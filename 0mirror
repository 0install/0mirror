#!/usr/bin/env python
# Copyright (C) 2007, Thomas Leonard
# See the COPYING file for details, or visit http://0install.net.

from optparse import OptionParser
import os, sys, time, shutil, subprocess, signal
from logging import info, debug, warn
from xml.dom import minidom

signal.alarm(10 * 60)		# Abort after 10 minutes

from zeroinstall import SafeException
from zeroinstall.injector.iface_cache import iface_cache
from zeroinstall.injector import basedir, model, namespaces, policy, handler, gpg, handler

from atom import AtomFeed, set_element

# Site configuration!
site_address = "http://roscidus.com/0mirror"

version = '0.1'

parser = OptionParser(usage="usage: %prog [options] PUBLIC-DIR")
parser.add_option("-v", "--verbose", help="more verbose output", action='count')
parser.add_option("-V", "--version", help="display version information", action='store_true')

(options, args) = parser.parse_args()

if options.version:
	print "0mirror (zero-install) " + version
	print "Copyright (C) 2007 Thomas Leonard"
	print "This program comes with ABSOLUTELY NO WARRANTY,"
	print "to the extent permitted by law."
	print "You may redistribute copies of this program"
	print "under the terms of the GNU General Public License."
	print "For more information about these matters, see the file named COPYING."
	sys.exit(0)

if options.verbose:
	import logging
	logger = logging.getLogger()
	if options.verbose == 1:
		logger.setLevel(logging.INFO)
	else:
		logger.setLevel(logging.DEBUG)

if len(args) != 1:
	parser.print_help()
	sys.exit(1)
public_dir = args[0]

feed_file = os.path.join(public_dir, 'feed-list')
ignore_file = os.path.join(public_dir, 'ignore-list')

def escape_slashes(path):
	return path.replace('/', '#')

def ensure_dirs(path):
	if not os.path.isdir(path):
		os.makedirs(path)

summary_xml = """
<summary type='xhtml'>
  <div xmlns="http://www.w3.org/1999/xhtml">
    <a href=""/> - <span/>
  </div>
</summary>
"""

unconfirmed_keys_xml = """
<summary type='xhtml'>
  <div xmlns="http://www.w3.org/1999/xhtml">
    New keys awaiting confirmation:
    <dl>
    </dl>
  </div>
</summary>
"""

warnings_xml = """
<summary type='xhtml'>
  <div xmlns="http://www.w3.org/1999/xhtml">
    <ul>
    </ul>
  </div>
</summary>
"""

unconfirmed_keys = []
class NonInteractiveHandler(handler.Handler):
	def confirm_trust_keys(self, interface, sigs, iface_xml):
		for x in sigs:
			warn("Need to check key %s for %s", x, interface)
			unconfirmed_keys.append((interface, x))
		return None
	
warnings = []
def add_warning(msg):
	warn(msg)
	warnings.append(msg)

key_dir = os.path.join(public_dir, 'keys')
ensure_dirs(key_dir)
keys = set()
def ensure_key(fingerprint):
	if fingerprint in keys:
		return
	key_path = os.path.join(key_dir, fingerprint[-16:] + '.gpg')
	child = subprocess.Popen(['gpg', '-a', '--export', fingerprint], stdout = subprocess.PIPE)
	keydata, unused = child.communicate()
	stream = file(key_path, 'w')
	stream.write(keydata)
	stream.close()
	print "Exported key", fingerprint
	keys.add(fingerprint)

ifaces = []

def format_date(date):
	return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(date))

def get_feed_dir(feed):
	if '#' in feed:
		raise SafeException("Invalid URL '%s'" % feed)
	scheme, rest = feed.split('://', 1)
	domain, rest = rest.split('/', 1)
	assert scheme in ('http', 'https', 'ftp')	# Just to check for mal-formed lines; add more as needed
	for x in [scheme, domain, rest]:
		if not x or x.startswith(','):
			raise SafeException("Invalid URL '%s'" % feed)
	return os.path.join('feeds', scheme, domain, escape_slashes(rest))

now = format_date(time.time())
news_feed = AtomFeed(title = "Zero Install News Feed",
			link = site_address + "/news-feed.xml",
			updated = now,
			author = "0mirror")

feed = None
try:
	if not os.path.isdir(public_dir):
		raise SafeException("Public directory '%s' does not exist. "
				    "To setup a new site, create it as an empty directory now." % public_dir)
	if not os.path.isfile(feed_file):
		raise SafeException("File '%s' does not exist. It should contain a list of feed URLs, one per line" % feed_file)
	print "Reading", feed_file

	feeds = filter(None, file(feed_file).read().split('\n'))
	feed_set = set(feeds)
	ignore_set = set(filter(None, file(ignore_file).read().split('\n')))
	known_set = feed_set | ignore_set

	handler = NonInteractiveHandler()
	for feed in feeds:
		info("Processing feed '%s'", feed)
		feed_dir = os.path.join(public_dir, get_feed_dir(feed))
		ensure_dirs(feed_dir)

		#print "Updating", feed
		p = policy.Policy(feed, handler)
		p.stale_feeds = set()
		iface = p.get_interface(feed)	# May start a download
		ifaces.append(iface)
		for x in p.stale_feeds:
			print "Updating stale feed", feed
			p.begin_iface_download(x)
		if handler.monitored_downloads:
			print "Waiting for downloads for", feed
			try:
				errors = handler.wait_for_downloads()
			except SafeException, ex:
				add_warning("Error updating '%s': %s" % (feed, str(ex)))
				continue
			for error in errors or []:
				add_warning("Error updating '%s': %s" % (feed, str(error)))

		cached = basedir.load_first_cache(namespaces.config_site, 'interfaces', model.escape(feed))
		if not cached:
			# Error during download?
			add_warning("Attempted to fetch '%s', but still not cached" % feed)
			continue

		for subfeed in iface.feeds:
			if subfeed.uri not in known_set:
				if subfeed.uri.startswith('/'):
					continue
				add_warning("WARNING: Subfeed %s of %s not in feeds list" % (subfeed.uri, iface.get_name()))

		# Check dependencies
		for impl in iface.implementations.values():
			if hasattr(impl, 'dependencies'):
				for dep in impl.dependencies.values():
					if dep.interface not in known_set:
						add_warning("Version %s of %s depends on %s, but that isn't being mirrored!" % (impl.get_version(), iface.get_name(), dep.interface))
						break
				else:
					continue
				break	# Once we've warned about one version, don't check any other versions

		style = os.path.join(feed_dir, 'interface.xsl')
		if not os.path.islink(style):
			os.symlink('../../../../feed_style.xsl', style)

		latest = os.path.join(feed_dir, 'latest.xml')

		last_modified = int(os.stat(cached).st_mtime)
		version_name = time.strftime('%Y-%m-%d_%H:%M.xml', time.gmtime(last_modified))
		version_path = os.path.join(feed_dir, version_name)

		if os.path.islink(latest) and os.readlink(latest) == version_name:
			if os.path.exists(version_path):
				continue
			warn("Broken symlink '%s'!", latest)

		# Get the keys
		stream = file(cached)
		unused, sigs = gpg.check_stream(stream)
		stream.close()

		for x in sigs:
			if isinstance(x, gpg.ValidSig):
				ensure_key(x.fingerprint)
			else:
				add_warning("Signature problem: %s" % x)

		shutil.copyfile(cached, version_path)
		latest_new = latest + '.new'
		if os.path.exists(latest_new):
			os.unlink(latest_new)
		os.symlink(version_name, latest_new)
		os.rename(latest_new, latest)
		print "Updated %s to %s" % (feed, version_name)
	
	feed = None
	
	if unconfirmed_keys:
		summary = minidom.parseString(unconfirmed_keys_xml)
		dl = summary.getElementsByTagNameNS("http://www.w3.org/1999/xhtml", "dl")[0]
		for iface, sig in unconfirmed_keys:
			dt = summary.createElementNS("http://www.w3.org/1999/xhtml", "dt")
			dl.appendChild(dt)
			dt.appendChild(summary.createTextNode(iface.uri))

			dd = summary.createElementNS("http://www.w3.org/1999/xhtml", "dd")
			dl.appendChild(dd)
			dd.appendChild(summary.createTextNode(str(sig)))

		news_feed.add_entry(title = "Keys awaiting confirmation",
			       link = site_address + "/news-feed.xml",
			       entry_id = "unconfirmed-keys",
			       updated = format_date(time.time()),
			       summary = summary.documentElement)

	if warnings:
		summary = minidom.parseString(warnings_xml)
		ul = summary.getElementsByTagNameNS("http://www.w3.org/1999/xhtml", "ul")[0]
		for warning in warnings:
			li = summary.createElementNS("http://www.w3.org/1999/xhtml", "li")
			ul.appendChild(li)
			li.appendChild(summary.createTextNode(warning))

		news_feed.add_entry(title = "Warnings",
			       link = site_address + "/news-feed.xml",
			       entry_id = "warnings",
			       updated = format_date(time.time()),
			       summary = summary.documentElement)

	latest_ifaces = [(iface.last_modified, iface) for iface in ifaces]
	latest_ifaces.sort()
	latest_ifaces = reversed(latest_ifaces[-16:])
	for date, iface in latest_ifaces:
		summary = minidom.parseString(summary_xml)
		set_element(summary, "summary/div/a", iface.get_name())
		set_element(summary, "summary/div/a/@href", iface.uri)
		set_element(summary, "summary/div/span", iface.summary)
		news_feed.add_entry(title = "%s feed updated" % iface.get_name(),
			       link = iface.uri,
			       extra_links = {'http://0install.net/2007/namespaces/0mirror/cached':
			       			site_address + "/" + get_feed_dir(iface.uri).replace('#', '%23') + "/latest.xml"},
			       entry_id = iface.uri,
			       updated = format_date(date),
			       summary = summary.documentElement)
	
	news_stream = file(os.path.join(public_dir, 'news-feed.xml'), 'w')
	news_feed.save(news_stream)
	news_stream.close()

except KeyboardInterrupt, ex:
	print >>sys.stderr, "Aborted at user's request"
	sys.exit(1)
except SafeException, ex:
	if options.verbose: raise
	print >>sys.stderr, ex
	if feed:
		print "(while processing %s)" % feed
	sys.exit(1)
