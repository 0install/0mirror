#!/usr/bin/env python
# Copyright (C) 2007, Thomas Leonard
# See the COPYING file for details, or visit http://0install.net.

from optparse import OptionParser
import os, sys, time, shutil, subprocess, signal, logging
from ConfigParser import RawConfigParser
from logging import info, debug, warn
from xml.dom import minidom
import codecs

signal.alarm(10 * 60)		# Abort after 10 minutes

from zeroinstall import SafeException
from zeroinstall.injector.iface_cache import iface_cache
from zeroinstall.injector import model, namespaces, config, gpg, handler
from zeroinstall.support import basedir, tasks

from atom import AtomFeed, set_element
from stats import Stats
from support import format_date, get_feed_dir, ensure_dirs

missing_set = set()

# Site configuration!

site_config_file = os.path.abspath('0mirror.ini')
FEED_TIMEOUT = 60	# Seconds to wait before giving up on a feed download

version = '0.1'

parser = OptionParser(usage="usage: %prog [options] PUBLIC-DIR")
parser.add_option("-v", "--verbose", help="more verbose output", action='count')
parser.add_option("-V", "--version", help="display version information", action='store_true')

(options, args) = parser.parse_args()

if options.version:
	print "0mirror (zero-install) " + version
	print "Copyright (C) 2010 Thomas Leonard"
	print "This program comes with ABSOLUTELY NO WARRANTY,"
	print "to the extent permitted by law."
	print "You may redistribute copies of this program"
	print "under the terms of the GNU General Public License."
	print "For more information about these matters, see the file named COPYING."
	sys.exit(0)

if options.verbose:
	logger = logging.getLogger()
	if options.verbose == 1:
		logger.setLevel(logging.INFO)
	else:
		logger.setLevel(logging.DEBUG)

if len(args) != 1:
	parser.print_help()
	sys.exit(1)

if not os.path.exists(site_config_file):
	print >>sys.stderr, "Configuration file '%s' not found!" % site_config_file
	sys.exit(1)
print "Reading configuration from", site_config_file

site_config = RawConfigParser()
site_config.read(site_config_file)

site_address = site_config.get('site', 'address') # e.g. "http://localhost/0mirror"
if not site_address.endswith('/'):
	site_address += '/'

# Where we try if the primary site fails
my_mirror = site_config.get('fetching', 'upstream_mirror') or None

n_feeds_to_update = int(site_config.get('fetching', 'n_feeds_to_update'))

public_dir = args[0]

feed_file = os.path.join(public_dir, 'feed-list')
ignore_file = os.path.join(public_dir, 'ignore-list')
warnings_file = os.path.join(public_dir, 'warnings.xml')

summary_xml = """
<summary type='xhtml'>
  <div xmlns="http://www.w3.org/1999/xhtml">
    <a href=""/> - <span/>
  </div>
</summary>
"""

warnings_xml = """
<summary type='xhtml'>
  <div xmlns="http://www.w3.org/1999/xhtml">
  </div>
</summary>
"""

unconfirmed_keys = [] # List of PendingFeeds
class NonInteractiveHandler(handler.Handler):
	def confirm_import_feed(self, pending, valid_sigs):
		for x in valid_sigs:
			warn("Need to check key %s for %s", x.fingerprint, pending.url)
			unconfirmed_keys.append(pending)
		return None

@tasks.async
def wait_with_timeout(delay, blocker):
	timeout = tasks.TimeoutBlocker(FEED_TIMEOUT, 'Mirror timeout')
	yield timeout, blocker
	tasks.check([timeout, blocker])
	if not blocker.happened:
		raise Exception("Timeout (waited %d seconds)" % delay)

warnings = []
def add_warning(title, msg):
	warn("%s: %s", title, msg)
	warnings.append((title, msg))

key_dir = os.path.join(public_dir, 'keys')
ensure_dirs(key_dir)
keys = set()
def ensure_key(fingerprint):
	if fingerprint in keys:
		return
	key_path = os.path.join(key_dir, fingerprint[-16:] + '.gpg')
	child = subprocess.Popen(['gpg', '-a', '--export', fingerprint], stdout = subprocess.PIPE)
	keydata, unused = child.communicate()
	stream = file(key_path, 'w')
	stream.write(keydata)
	stream.close()
	print "Exported key", fingerprint
	keys.add(fingerprint)

feeds = []

now = format_date(time.time())
news_feed = AtomFeed(title = "Zero Install News Feed",
			link = site_address + "/news-feed.xml",
			updated = now,
			author = "0mirror")
warnings_feed = AtomFeed(title = "0mirror Warnings Feed",
			link = site_address + "/warnings.xml",
			updated = now,
			author = "0mirror",
			source = warnings_file)

def load_feed(url):
	return iface_cache.get_feed(url)

def load_feeds(feed_uris):
	logging.getLogger("0install").setLevel(logging.ERROR)
	try:
		feeds = {}

		for feed_url in feed_uris:
			feeds[feed_url] = load_feed(feed_url)
		return feeds
	finally:
		logging.getLogger("0install").setLevel(logging.WARNING)

feed = None
try:
	stats = Stats()
	if not os.path.isdir(public_dir):
		raise SafeException("Public directory '%s' does not exist. "
				    "To setup a new site, create it as an empty directory now." % public_dir)
	if not os.path.isfile(feed_file):
		raise SafeException("File '%s' does not exist. It should contain a list of feed URLs, one per line" % feed_file)
	print "Reading", feed_file

	lines = filter(None, file(feed_file).read().split('\n'))
	feed_uris = [line for line in lines if not line.startswith('-')]
	feed_set = set(feed_uris)
	ignore_set = set(filter(None, file(ignore_file).read().split('\n')))
	inactive_set = set(line[1:] for line in lines if line.startswith('-'))

	known_set = feed_set | inactive_set

	stale_feeds = []	# [(last-checked, feed)]

	c = config.load_config()
	c.feed_mirror = my_mirror

	feeds = load_feeds(feed_uris)

	def last_checked(feed):
		if feed is None:
			# If we've never downloaded this feed, just keep trying (ignore last_check_attempt)
			return 0
		# Use the latest of the last successful check or the last failed check
		last_check_attempt = iface_cache.get_last_check_attempt(feed.url)
		if not last_check_attempt:
			return feed.last_checked
		return max(feed.last_checked or 0, last_check_attempt)

	# List all the feeds, starting with the most stale
	stale_feeds = [(last_checked(feed), url, feed) for url, feed in feeds.items()]
	stale_feeds.sort()

	# If we've got some completely new feeds, update all of them now
	while n_feeds_to_update < len(stale_feeds) and stale_feeds[n_feeds_to_update - 1][0] in (0, None):
		n_feeds_to_update += 1

	# Update the first few feeds in the list
	stale_feeds = stale_feeds[:n_feeds_to_update]
	for last_check, feed_url, feed in stale_feeds:
		if last_check > 0:
			ctime_str = time.strftime('%Y-%m-%d_%H:%M', time.gmtime(last_check))
			print "Feed %s last checked %s; updating..." % (feed_url, ctime_str)
		else:
			print "Feed %s is new; fetching..." % feed_url

		iface_cache.mark_as_checking(feed_url)
		blocker = c.fetcher.download_and_import_feed(feed_url, iface_cache)
		try:
			tasks.wait_for_blocker(wait_with_timeout(FEED_TIMEOUT, blocker))
		except Exception, ex:
			add_warning("Error fetching feed", "Error fetching '%s': %s" % (feed_url, ex))
			continue
		# Reload
		feed = feeds[feed_url] = load_feed(feed_url)
		#assert feed.last_checked, feed

	for feed_url in feed_uris:
		info("Processing feed '%s'", feed_url)
		feed = feeds[feed_url]
		if feed is None:
			# Error during download?
			add_warning("Fetch failed", "Attempted to fetch '%s', but still not cached" % feed_url)
			continue

		feed_dir = os.path.join(public_dir, get_feed_dir(feed_url))
		ensure_dirs(feed_dir)

		cached = basedir.load_first_cache(namespaces.config_site, 'interfaces', model.escape(feed.url))
		assert cached is not None

		for subfeed in feed.feeds:
			if subfeed.uri not in known_set:
				if subfeed.uri.startswith('/'):
					continue
				if subfeed.uri not in ignore_set:
					add_warning("Missing subfeed", "WARNING: Subfeed %s of %s not in feeds list" % (subfeed.uri, feed.get_name()))

		# Check dependencies
		for impl in feed.implementations.values():
			for dep in impl.requires:
				if dep.interface not in known_set and dep.interface not in missing_set:
					add_warning("Missing dependency", "Version %s of %s depends on %s, but that isn't being mirrored!" % (impl.get_version(), feed.url, dep.interface))
					missing_set.add(dep.interface)
			else:
				continue
			break	# Once we've warned about one version, don't check any other versions

		style = os.path.join(feed_dir, 'interface.xsl')
		if not os.path.islink(style):
			os.symlink('../../../../feed_style.xsl', style)

		latest = os.path.join(feed_dir, 'latest.xml')

		last_modified = int(os.stat(cached).st_mtime)
		version_name = time.strftime('%Y-%m-%d_%H:%M.xml', time.gmtime(last_modified))
		version_path = os.path.join(feed_dir, version_name)

		if os.path.islink(latest) and os.readlink(latest) == version_name:
			if os.path.exists(version_path):
				continue
			warn("Broken symlink '%s'!", latest)

		# Get the keys
		stream = file(cached)
		unused, sigs = gpg.check_stream(stream)
		stream.close()

		for x in sigs:
			if isinstance(x, gpg.ValidSig):
				ensure_key(x.fingerprint)
			else:
				add_warning("Signature problem", x)

		shutil.copyfile(cached, version_path)
		latest_new = latest + '.new'
		if os.path.exists(latest_new):
			os.unlink(latest_new)
		os.symlink(version_name, latest_new)
		os.rename(latest_new, latest)
		print "Updated %s to %s" % (feed, version_name)

	feed = None

	for feed_url in known_set:
		feed = load_feed(feed_url)
		if feed and feed.last_modified:
			stats.add_feed(feed, feed_url in feed_set)

	stats.write_summary(public_dir)

	for pending_feed in unconfirmed_keys:
		add_warning("Key awaiting confirmation",
			    "Feed: {feed}, Fingerprint: {fingerprint}".format(
				feed = pending_feed.url,
				fingerprint = pending_feed.sigs[0].fingerprint))

	if warnings:
		i = 0
		for (title, warning) in warnings:
			summary = minidom.parseString(warnings_xml)
			div = summary.getElementsByTagNameNS("http://www.w3.org/1999/xhtml", "div")[0]
			div.appendChild(summary.createTextNode(warning))
			warnings_feed.add_entry(title = title,
				       link = site_address + "/warnings.xml",
				       entry_id = "warning-" + now + '-%d' % i,
				       updated = now,
				       summary = summary.documentElement)
			i += 1
		warnings_feed.limit(20)
		with open(warnings_file, 'w') as stream:
			warnings_feed.save(stream)

	latest_feeds = [(feed.last_modified, feed) for feed in feeds.values() if feed]
	latest_feeds.sort()
	latest_feeds = reversed(latest_feeds[-16:])
	for date, feed in latest_feeds:
		summary = minidom.parseString(summary_xml)
		set_element(summary, "summary/div/a", feed.get_name())
		local_html_page = site_address + "/" + get_feed_dir(feed.url).replace('#', '%23') + "/feed.html"
		set_element(summary, "summary/div/a/@href", local_html_page)
		set_element(summary, "summary/div/span", feed.summary)
		news_feed.add_entry(title = "%s feed updated" % feed.get_name(),
			       link = local_html_page,
			       entry_id = feed.url,
			       updated = format_date(date),
			       summary = summary.documentElement)

	news_stream = codecs.open(os.path.join(public_dir, 'news-feed.xml'), 'w', encoding = 'utf-8')
	news_feed.save(news_stream)
	news_stream.close()

	if False:
		# Warn about possible missing feeds...
		child = subprocess.Popen(['0launch', '--list'], stdout = subprocess.PIPE)
		all_feeds, unused = child.communicate()
		all_feeds = set([x for x in all_feeds.split('\n') if x and not x.startswith('/')])
		unknown = all_feeds - known_set

		if unknown:
			print "\nUnknown feeds (add to known or ignore lists):"
			for feed in sorted(unknown):
				if '/tests/' in feed: continue
				print feed

	if missing_set:
		print "\nMissing feeds:"
		for x in missing_set:
			print x

except KeyboardInterrupt, ex:
	print >>sys.stderr, "Aborted at user's request"
	sys.exit(1)
except SafeException, ex:
	if options.verbose: raise
	print >>sys.stderr, ex
	if feed:
		print "(while processing %s)" % feed
	sys.exit(1)
