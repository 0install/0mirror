#!/usr/bin/env python
# Copyright (C) 2007, Thomas Leonard
# See the COPYING file for details, or visit http://0install.net.

from optparse import OptionParser
import os, sys, time, shutil, subprocess, signal
from ConfigParser import RawConfigParser
from logging import info, debug, warn
from xml.dom import minidom

signal.alarm(10 * 60)		# Abort after 10 minutes

from zeroinstall import SafeException
from zeroinstall.injector.iface_cache import iface_cache
from zeroinstall.injector import model, namespaces, policy, handler, gpg, handler, qdom, distro
from zeroinstall.support import basedir, tasks

from atom import AtomFeed, set_element
from stats import Stats
from support import format_date, get_feed_dir, ensure_dirs

# Site configuration!

site_config_file = os.path.abspath('0mirror.ini')
FEED_TIMEOUT = 60	# Seconds to wait before giving up on a feed download

version = '0.1'

parser = OptionParser(usage="usage: %prog [options] PUBLIC-DIR")
parser.add_option("-v", "--verbose", help="more verbose output", action='count')
parser.add_option("-V", "--version", help="display version information", action='store_true')

(options, args) = parser.parse_args()

if options.version:
	print "0mirror (zero-install) " + version
	print "Copyright (C) 2010 Thomas Leonard"
	print "This program comes with ABSOLUTELY NO WARRANTY,"
	print "to the extent permitted by law."
	print "You may redistribute copies of this program"
	print "under the terms of the GNU General Public License."
	print "For more information about these matters, see the file named COPYING."
	sys.exit(0)

if options.verbose:
	import logging
	logger = logging.getLogger()
	if options.verbose == 1:
		logger.setLevel(logging.INFO)
	else:
		logger.setLevel(logging.DEBUG)

if len(args) != 1:
	parser.print_help()
	sys.exit(1)

if not os.path.exists(site_config_file):
	print >>sys.stderr, "Configuration file '%s' not found!" % site_config_file
	sys.exit(1)
print "Reading configuration from", site_config_file

site_config = RawConfigParser()
site_config.read(site_config_file)

site_address = site_config.get('site', 'address') # e.g. "http://localhost/0mirror"
if not site_address.endswith('/'):
	site_address += '/'

# Where we try if the primary site fails
my_mirror = site_config.get('fetching', 'upstream_mirror') or None

public_dir = args[0]

feed_file = os.path.join(public_dir, 'feed-list')
ignore_file = os.path.join(public_dir, 'ignore-list')

summary_xml = """
<summary type='xhtml'>
  <div xmlns="http://www.w3.org/1999/xhtml">
    <a href=""/> - <span/>
  </div>
</summary>
"""

unconfirmed_keys_xml = """
<summary type='xhtml'>
  <div xmlns="http://www.w3.org/1999/xhtml">
    New keys awaiting confirmation:
    <dl>
    </dl>
  </div>
</summary>
"""

warnings_xml = """
<summary type='xhtml'>
  <div xmlns="http://www.w3.org/1999/xhtml">
    <ul>
    </ul>
  </div>
</summary>
"""

unconfirmed_keys = [] # List of PendingFeeds
class NonInteractiveHandler(handler.Handler):
	def confirm_import_feed(self, pending, valid_sigs):
		for x in valid_sigs:
			warn("Need to check key %s for %s", x.fingerprint, pending.url)
			unconfirmed_keys.append(pending)
		return None

@tasks.async
def wait_with_timeout(delay, blocker):
	timeout = tasks.TimeoutBlocker(FEED_TIMEOUT, 'Mirror timeout')
	yield timeout, blocker
	tasks.check(timeout, blocker)
	if not blocker.happened:
		raise Exception("Timeout (waited %d seconds)" % delay)

warnings = []
def add_warning(msg):
	warn(msg)
	warnings.append(msg)

key_dir = os.path.join(public_dir, 'keys')
ensure_dirs(key_dir)
keys = set()
def ensure_key(fingerprint):
	if fingerprint in keys:
		return
	key_path = os.path.join(key_dir, fingerprint[-16:] + '.gpg')
	child = subprocess.Popen(['gpg', '-a', '--export', fingerprint], stdout = subprocess.PIPE)
	keydata, unused = child.communicate()
	stream = file(key_path, 'w')
	stream.write(keydata)
	stream.close()
	print "Exported key", fingerprint
	keys.add(fingerprint)

feeds = []

now = format_date(time.time())
news_feed = AtomFeed(title = "Zero Install News Feed",
			link = site_address + "/news-feed.xml",
			updated = now,
			author = "0mirror")

fake_distro = distro.Distribution()

def load_feed(url):
	cached = basedir.load_first_cache(namespaces.config_site, 'interfaces', model.escape(url))
	if cached:
		with open(cached) as stream:
			root = qdom.parse(stream)
			feed = model.ZeroInstallFeed(root, None, fake_distro)
			feed.last_modified = int(os.stat(cached).st_mtime)

			# We don't currently have a clean way to get the last checked time using
			# the library...
			user = basedir.load_first_config(namespaces.config_site, 'injector',
							   'user_overrides', model.escape(url))
			if user:
				root = qdom.parse(file(user))
				last_checked = root.getAttribute('last-checked')
				if last_checked:
					feed.last_checked = int(last_checked)

			return feed
	return None

feed = None
try:
	stats = Stats()
	if not os.path.isdir(public_dir):
		raise SafeException("Public directory '%s' does not exist. "
				    "To setup a new site, create it as an empty directory now." % public_dir)
	if not os.path.isfile(feed_file):
		raise SafeException("File '%s' does not exist. It should contain a list of feed URLs, one per line" % feed_file)
	print "Reading", feed_file

	lines = filter(None, file(feed_file).read().split('\n'))
	feed_uris = [line for line in lines if not line.startswith('-')]
	feed_set = set(feed_uris)
	ignore_set = set(filter(None, file(ignore_file).read().split('\n')))
	inactive_set = set(line[1:] for line in lines if line.startswith('-'))

	known_set = feed_set | inactive_set

	handler = NonInteractiveHandler()
	for feed_url in feed_uris:
		info("Processing feed '%s'", feed_url)
		feed_dir = os.path.join(public_dir, get_feed_dir(feed_url))
		ensure_dirs(feed_dir)

		#print "Updating", feed_url
		p = policy.Policy(feed_url, handler)
		p.fetcher.feed_mirror = my_mirror
		feed = load_feed(feed_url)
		if p.is_stale(feed):
			if feed and feed.last_modified:
				ctime_str = time.strftime('%Y-%m-%d_%H:%M', time.gmtime(feed.last_checked))
				print "Feed %s last checked %s; updating..." % (feed_url, ctime_str)
			else:
				print "Feed %s is new; fetching..." % feed_url
			blocker = p.fetcher.download_and_import_feed(feed_url, iface_cache)
			try:
				handler.wait_for_blocker(wait_with_timeout(FEED_TIMEOUT, blocker))
			except Exception, ex:
				add_warning("Error fetching '%s': %s" % (feed_url, ex))
				continue
			# Reload
			feed = load_feed(feed_url)
			if not feed:
				# Error during download?
				add_warning("Attempted to fetch '%s', but still not cached" % feed_url)
				continue
		feeds.append(feed)

		cached = basedir.load_first_cache(namespaces.config_site, 'interfaces', model.escape(feed.url))
		assert cached is not None

		for subfeed in feed.feeds:
			if subfeed.uri not in known_set:
				if subfeed.uri.startswith('/'):
					continue
				if subfeed.uri not in ignore_set:
					add_warning("WARNING: Subfeed %s of %s not in feeds list" % (subfeed.uri, feed.get_name()))

		# Check dependencies
		for impl in feed.implementations.values():
			for dep in impl.requires:
				if dep.interface not in known_set:
					add_warning("Version %s of %s depends on %s, but that isn't being mirrored!" % (impl.get_version(), feed.get_name(), dep.interface))
					break
			else:
				continue
			break	# Once we've warned about one version, don't check any other versions

		style = os.path.join(feed_dir, 'interface.xsl')
		if not os.path.islink(style):
			os.symlink('../../../../feed_style.xsl', style)

		latest = os.path.join(feed_dir, 'latest.xml')

		last_modified = int(os.stat(cached).st_mtime)
		version_name = time.strftime('%Y-%m-%d_%H:%M.xml', time.gmtime(last_modified))
		version_path = os.path.join(feed_dir, version_name)

		if os.path.islink(latest) and os.readlink(latest) == version_name:
			if os.path.exists(version_path):
				continue
			warn("Broken symlink '%s'!", latest)

		# Get the keys
		stream = file(cached)
		unused, sigs = gpg.check_stream(stream)
		stream.close()

		for x in sigs:
			if isinstance(x, gpg.ValidSig):
				ensure_key(x.fingerprint)
			else:
				add_warning("Signature problem: %s" % x)

		shutil.copyfile(cached, version_path)
		latest_new = latest + '.new'
		if os.path.exists(latest_new):
			os.unlink(latest_new)
		os.symlink(version_name, latest_new)
		os.rename(latest_new, latest)
		print "Updated %s to %s" % (feed, version_name)

	feed = None

	for feed_url in known_set:
		feed = load_feed(feed_url)
		if feed and feed.last_modified:
			stats.add_feed(feed, feed_url in feed_set)

	stats.write_summary(public_dir)

	if unconfirmed_keys:
		summary = minidom.parseString(unconfirmed_keys_xml)
		dl = summary.getElementsByTagNameNS("http://www.w3.org/1999/xhtml", "dl")[0]
		for pending_feed in unconfirmed_keys:
			dt = summary.createElementNS("http://www.w3.org/1999/xhtml", "dt")
			dl.appendChild(dt)
			dt.appendChild(summary.createTextNode(pending_feed.url))

			dd = summary.createElementNS("http://www.w3.org/1999/xhtml", "dd")
			dl.appendChild(dd)
			dd.appendChild(summary.createTextNode(str(pending_feed.sigs[0].fingerprint)))

		news_feed.add_entry(title = "Keys awaiting confirmation",
			       link = site_address + "/news-feed.xml",
			       entry_id = "unconfirmed-keys",
			       updated = format_date(time.time()),
			       summary = summary.documentElement)

	if warnings:
		summary = minidom.parseString(warnings_xml)
		ul = summary.getElementsByTagNameNS("http://www.w3.org/1999/xhtml", "ul")[0]
		for warning in warnings:
			li = summary.createElementNS("http://www.w3.org/1999/xhtml", "li")
			ul.appendChild(li)
			li.appendChild(summary.createTextNode(warning))

		news_feed.add_entry(title = "Warnings",
			       link = site_address + "/news-feed.xml",
			       entry_id = "warnings",
			       updated = format_date(time.time()),
			       summary = summary.documentElement)

	latest_feeds = [(feed.last_modified, feed) for feed in feeds]
	latest_feeds.sort()
	latest_feeds = reversed(latest_feeds[-16:])
	for date, feed in latest_feeds:
		summary = minidom.parseString(summary_xml)
		set_element(summary, "summary/div/a", feed.get_name())
		local_html_page = site_address + "/" + get_feed_dir(feed.url).replace('#', '%23') + "/feed.html"
		set_element(summary, "summary/div/a/@href", local_html_page)
		set_element(summary, "summary/div/span", feed.summary)
		news_feed.add_entry(title = "%s feed updated" % feed.get_name(),
			       link = local_html_page,
			       entry_id = feed.url,
			       updated = format_date(date),
			       summary = summary.documentElement)

	news_stream = file(os.path.join(public_dir, 'news-feed.xml'), 'w')
	news_feed.save(news_stream)
	news_stream.close()

	if False:
		# Warn about possible missing feeds...
		child = subprocess.Popen(['0launch', '--list'], stdout = subprocess.PIPE)
		all_feeds, unused = child.communicate()
		all_feeds = set([x for x in all_feeds.split('\n') if x and not x.startswith('/')])
		unknown = all_feeds - known_set

		if unknown:
			print "\nUnknown feeds (add to known or ignore lists):"
			for feed in sorted(unknown):
				if '/tests/' in feed: continue
				print feed


except KeyboardInterrupt, ex:
	print >>sys.stderr, "Aborted at user's request"
	sys.exit(1)
except SafeException, ex:
	if options.verbose: raise
	print >>sys.stderr, ex
	if feed:
		print "(while processing %s)" % feed
	sys.exit(1)
